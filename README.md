# ML-Pipeline

This project implements an end-to-end Machine Learning Pipeline for text classification, likely spam detection, using DVC (Data Version Control) for experiment tracking and reproducibility. The pipeline includes data ingestion, preprocessing, feature engineering, model building, and model evaluation.

## Project Structure

-   `.git/`: Git version control.
-   `data/`: Contains raw, interim, and processed data.
    -   `raw/`: Raw ingested data (e.g., `train.csv`, `test.csv`).
    -   `interim/`: Data after initial preprocessing (e.g., `train_processed.csv`, `test_processed.csv`).
    -   `processed/`: Data after feature engineering (e.g., `train_tfidf.csv`, `test_tfidf.csv`).
-   `dvc.lock`: DVC lock file, tracking data and model versions.
-   `dvc.yaml`: Defines the DVC pipeline stages and dependencies.
-   `dvclive/`: DVC Live directory for experiment tracking, metrics, and plots.
-   `experiments/`: Directory for storing experiment-related artifacts (e.g., configurations, logs).
-   `LICENSE`: Project license file.
-   `logs/`: Contains logs generated by various pipeline components.
-   `models/`: Stores trained machine learning models (e.g., `model.pkl`).
-   `params.yaml`: Configuration parameters for the pipeline stages, such as `test_size` for data ingestion, `max_features` for feature engineering, and `n_estimators`, `random_state` for model building.
-   `README.md`: This file.
-   `reports/`: Stores evaluation reports and metrics (e.g., `metrics.json`).
-   `src/`: Contains the Python scripts for each stage of the ML pipeline.
    -   `data_ingestion.py`: Handles loading raw data and splitting it into training and testing sets.
    -   `preprocessing.py`: Performs text preprocessing steps like lowercasing, tokenization, stop word removal, and stemming.
    -   `feature_engineering.py`: Applies feature engineering techniques, specifically TF-IDF vectorization.
    -   `model_building.py`: Trains the machine learning model (RandomForestClassifier).
    -   `model_evaluation.py`: Evaluates the trained model and logs metrics.

## ML Pipeline Workflow

The pipeline is orchestrated using DVC, ensuring reproducibility and versioning of data and models. The `dvc.yaml` file defines the following stages:

1.  **Data Ingestion (`data_ingestion.py`)**:
    -   Loads the raw dataset from a specified URL.
    -   Renames columns and drops unnecessary ones.
    -   Splits the data into training and testing sets based on `data_ingestion.test_size` from `params.yaml`.
    -   Saves the raw `train.csv` and `test.csv` to `data/raw/`.

2.  **Data Preprocessing (`preprocessing.py`)**:
    -   Loads the raw training and testing data.
    -   Applies text transformation (lowercasing, tokenization, stop word removal, stemming).
    -   Encodes the target variable (e.g., using `LabelEncoder`).
    -   Removes duplicate entries.
    -   Saves the processed data as `train_processed.csv` and `test_processed.csv` to `data/interim/`.

3.  **Feature Engineering (`feature_engineering.py`)**:
    -   Loads the preprocessed data.
    -   Applies TF-IDF vectorization to the text data, using `feature_engineering.max_features` from `params.yaml`.
    -   Saves the TF-IDF transformed data as `train_tfidf.csv` and `test_tfidf.csv` to `data/processed/`.

4.  **Model Building (`model_building.py`)**:
    -   Loads the TF-IDF transformed training data.
    -   Trains a `RandomForestClassifier` model using parameters like `n_estimators` and `random_state` from `params.yaml`.
    -   Saves the trained model as `model.pkl` to the `models/` directory.

5.  **Model Evaluation (`model_evaluation.py`)**:
    -   Loads the trained model and the TF-IDF transformed test data.
    -   Evaluates the model using metrics suchs as accuracy, precision, recall, and ROC AUC.
    -   Logs metrics using DVC Live for experiment tracking.
    -   Saves the evaluation metrics to `reports/metrics.json`.

## Setup and Usage

1.  **Clone the repository:**

    ```bash
    git clone git@github.com:sakethlingerker/ML-Pipeline.git
    ```

2.  **Install dependencies:**

    (Assuming you have a `requirements.txt` or similar file, otherwise, you would install them manually based on the imports in the `src` files. Common ones include `pandas`, `scikit-learn`, `PyYAML`, `dvc`, `dvclive`, `nltk`.)

    ```bash
    pip install -r requirements.txt
    ```

3.  **Initialize DVC (if not already initialized):**

    ```bash
    dvc init
    ```

4.  **Run the pipeline:**

    ```bash
    dvc repro
    ```

    This command will execute all stages defined in `dvc.yaml` in the correct order, ensuring data and model versioning.

5.  **View metrics and plots:**

    You can view the tracked metrics and plots using DVC commands, typically by navigating to the `dvclive` directory or using DVC Studio if configured.

    ```bash
    dvc metrics show
    dvc plots show
    ```

## Configuration

Adjust pipeline parameters in `params.yaml`:

```yaml
# params.yaml
data_ingestion:
  test_size: 0.30

feature_engineering:
  max_features: 50

model_building:
  n_estimators: 25
  random_state: 2
```

Modify these values to experiment with different data splits, feature engineering parameters, and model hyperparameters.

## Logging

Each script in the `src/` directory is configured to log its operations to the `logs/` directory, providing detailed information about the pipeline's execution.